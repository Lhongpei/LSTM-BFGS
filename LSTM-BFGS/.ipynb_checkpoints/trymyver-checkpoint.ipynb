{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy \n",
    "from timeit import default_timer as timer\n",
    "import torchmin.bfgs as bfs\n",
    "import torchmin.line_search as ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "DIM = 10\n",
    "batchsize = 1\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA=True\n",
    "    device=torch.cuda.device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    "\n",
    "######################    手工的优化器   ###################\n",
    "######################    hand-craft optimizer   ###################\n",
    "# def update(B,gradients,precondition):\n",
    "    \n",
    "def SGD(gradients, state, learning_rate=0.001):\n",
    "   \n",
    "    return -gradients*learning_rate, state\n",
    "\n",
    "def RMS(gradients, state, learning_rate=0.01, decay_rate=0.9):\n",
    "    if state is None:\n",
    "        state = torch.zeros(DIM)\n",
    "        if USE_CUDA == True:\n",
    "            state = state.cuda()\n",
    "            \n",
    "    state = decay_rate*state + (1-decay_rate)*torch.pow(gradients, 2)\n",
    "    update = -learning_rate*gradients / (torch.sqrt(state+1e-5))\n",
    "    return update, state\n",
    "\n",
    "\n",
    "def adam():\n",
    "    return torch.optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(W,Y,x):\n",
    "    \"\"\"quadratic function : f(\\theta) = \\|W\\theta - y\\|_2^2\"\"\"\n",
    "    if USE_CUDA:\n",
    "        W = W.cuda()\n",
    "        Y = Y.cuda()\n",
    "        x = x.cuda()\n",
    "\n",
    "    return ((torch.matmul(W,x.unsqueeze(-1)).squeeze()-Y)**2).sum(dim=1).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "#####################    自动 LSTM 优化器模型  ##########################\n",
    "#####################    auto LSTM optimizer model  ##########################\n",
    "class LSTM_optimizer_Model(torch.nn.Module):\n",
    "    \"\"\"LSTM优化器\n",
    "       LSTM optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = 10 ,output_scale = 1):\n",
    "        super(LSTM_optimizer_Model,self).__init__()\n",
    "        self.preprocess_flag = preprocess\n",
    "        self.p = p\n",
    "        self.input_flag = 2\n",
    "        if preprocess != True:\n",
    "             self.input_flag = 1\n",
    "        self.output_scale = output_scale \n",
    "        self.lstm = torch.nn.LSTM(input_size*self.input_flag, hidden_size, num_stacks)\n",
    "        self.Linear = torch.nn.Linear(hidden_size,output_size) #1-> output_size\n",
    "        \n",
    "    def LogAndSign_Preprocess_Gradient(self,gradients):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.\n",
    "          p       : `p` > 0 is a parameter controlling how small gradients are disregarded \n",
    "        Returns:\n",
    "          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements\n",
    "          along the nth dimension correspond to the `log output` \\in [-1,1] and the remaining\n",
    "          `d_n` elements to the `sign output`.\n",
    "        \"\"\"\n",
    "        p  = self.p\n",
    "        log = torch.log(torch.abs(gradients))\n",
    "        clamp_log = torch.clamp(log/p , min = -1.0,max = 1.0)\n",
    "        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = -1.0, max =1.0)\n",
    "        return torch.cat((clamp_log,clamp_sign),dim = -1) #在gradients的最后一维input_dims拼接 # concatenate in final dim\n",
    "    \n",
    "    def Output_motification_And_Update_LSTM_Hidden_State(self, input_gradients, prev_state):\n",
    "        \"\"\"LSTM的核心操作  core operation\n",
    "        coordinate-wise LSTM \"\"\"\n",
    "        if prev_state is None: #init_state\n",
    "            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),\n",
    "                            torch.zeros(Layers,batchsize,Hidden_nums))\n",
    "            if USE_CUDA :\n",
    "                 prev_state = (torch.zeros(Layers,batchsize,Hidden_nums).cuda(),\n",
    "                            torch.zeros(Layers,batchsize,Hidden_nums).cuda())\n",
    "         \t\t\t\n",
    "        update , next_state = self.lstm(input_gradients, prev_state)\n",
    "        update = self.Linear(update) * self.output_scale # transform the LSTM output to the target output dim \n",
    "        return update, next_state\n",
    "    \n",
    "    def forward(self,input_data, prev_state):\n",
    "        if USE_CUDA:\n",
    "            move_gradients = move_gradients.cuda()\n",
    "            move = move.cuda()\n",
    "        #pytorch requires the `torch.nn.lstm`'s input as（1，batchsize,input_dim）\n",
    "        # original gradient.size()=torch.size[5] ->[1,1,5]\n",
    "        # gradients = move_gradients.unsqueeze(0)\n",
    "        # moves = move.unsqueeze(0)\n",
    "        # combined = torch.cat((moves, gradients), dim=0)\n",
    "        # if self.preprocess_flag == True:\n",
    "        #     gradients = self.LogAndSign_Preprocess_Gradient(gradients)\n",
    "        update , next_state = self.Output_motification_And_Update_LSTM_Hidden_State(input_data , prev_state)\n",
    "        # Squeeze to make it a single batch again.[1,1,5]->[5]\n",
    "        update = update.squeeze().squeeze()\n",
    "       \n",
    "        return update , next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_optimizer_Model(\n",
      "  (lstm): LSTM(20, 20, num_layers=2)\n",
      "  (Linear): Linear(in_features=20, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Layers = 2\n",
    "Hidden_nums = 20\n",
    "Input_DIM = DIM*2\n",
    "Output_DIM = DIM**2\n",
    "output_scale_value=1\n",
    "\n",
    "#######   构造一个优化器  #######\n",
    "#######  construct a optimizer  #######\n",
    "LSTM_optimizer = LSTM_optimizer_Model(Input_DIM, Output_DIM, Hidden_nums ,Layers , batchsize=batchsize,\\\n",
    "                preprocess=False,output_scale=output_scale_value)\n",
    "print(LSTM_optimizer)\n",
    "\n",
    "if USE_CUDA:\n",
    "    LSTM_optimizer = LSTM_optimizer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Learner( object ):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        `f` : 要学习的问题 the learning problem, also called `optimizee` in the paper\n",
    "        `optimizer` : 使用的优化器 the used optimizer \n",
    "        `train_steps` : 对于其他SGD,Adam等是训练周期，对于LSTM训练时的展开周期  training steps for SGD and ADAM, unfolded step for LSTM train\n",
    "        `retain_graph_flag=False`  : 默认每次loss_backward后 释放动态图 default: free the dynamic graph after the loss backward\n",
    "        `reset_theta = False `  :  默认每次学习前 不随机初始化参数 default: do not initialize the theta\n",
    "        `reset_function_from_IID_distirbution = True` : 默认从分布中随机采样函数  default: random sample from  distribution\n",
    "\n",
    "    Return :\n",
    "        `losses` : reserves each loss value in each iteration\n",
    "        `global_loss_graph` : constructs the graph of all Unroll steps for LSTM's BPTT \n",
    "    \"\"\"\n",
    "    def __init__(self,    f ,   optimizer,  train_steps ,  \n",
    "                                            eval_flag = False,\n",
    "                                            retain_graph_flag=False,\n",
    "                                            reset_theta = False ,\n",
    "                                            reset_function_from_IID_distirbution = True):\n",
    "        self.f = f\n",
    "        self.optimizer = optimizer\n",
    "        self.train_steps = train_steps\n",
    "        #self.num_roll=num_roll\n",
    "        self.eval_flag = eval_flag\n",
    "        self.retain_graph_flag = retain_graph_flag\n",
    "        self.reset_theta = reset_theta\n",
    "        self.reset_function_from_IID_distirbution = reset_function_from_IID_distirbution  \n",
    "        self.init_theta_of_f()\n",
    "        self.state = None\n",
    "        self.global_loss_graph = 0 # global loss for optimizing LSTM\n",
    "        self.losses = []   # KEEP each loss of all epoches\n",
    "\n",
    "    def init_theta_of_f(self,):  \n",
    "        ''' 初始化 优化问题 f 的参数 \n",
    "            initialize the theta of optimization f '''\n",
    "        self.DIM = 10\n",
    "        self.batchsize = 128\n",
    "        self.W = torch.randn(batchsize,DIM,DIM) # represents IID\n",
    "        self.Y = torch.randn(batchsize,DIM)\n",
    "        self.x = torch.zeros(self.batchsize,self.DIM)\n",
    "        self.x.requires_grad = True\n",
    "        if USE_CUDA:\n",
    "            self.W = self.W.cuda()\n",
    "            self.Y = self.Y.cuda()\n",
    "            self.x = self.x.cuda()\n",
    "        \n",
    "        \n",
    "            \n",
    "    def Reset_Or_Reuse(self , x , W , Y , state, num_roll):\n",
    "        ''' re-initialize the `W, Y, x , state`  at the begining of each global training\n",
    "            IF `num_roll` == 0    '''\n",
    "\n",
    "        reset_theta =self.reset_theta\n",
    "        reset_function_from_IID_distirbution = self.reset_function_from_IID_distirbution\n",
    "\n",
    "       \n",
    "        if num_roll == 0 and reset_theta == True:\n",
    "            theta = torch.zeros(batchsize,DIM)\n",
    "            theta_init_new =  theta.clone().detach().requires_grad_(True)\n",
    "            x = theta_init_new\n",
    "            \n",
    "            \n",
    "        ################   每次全局训练迭代，从独立同分布的Normal Gaussian采样函数     ##################\n",
    "        ################   at the first iteration , sample from IID Normal Gaussian    ##################\n",
    "        if num_roll == 0 and reset_function_from_IID_distirbution == True :\n",
    "            W = torch.randn(batchsize,DIM,DIM) # represents IID\n",
    "            Y = torch.randn(batchsize,DIM)     # represents IID\n",
    "         \n",
    "            \n",
    "        if num_roll == 0:\n",
    "            state = None\n",
    "            print('reset the values of `W`, `x`, `Y` and `state` for this optimizer')\n",
    "            \n",
    "        if USE_CUDA:\n",
    "            W = W.cuda()\n",
    "            Y = Y.cuda()\n",
    "            x = x.cuda()\n",
    "            x.retain_grad()\n",
    "          \n",
    "            \n",
    "        return  x , W , Y , state\n",
    "\n",
    "    def __call__(self, num_roll=0) : \n",
    "        '''\n",
    "        Total Training steps = Unroll_Train_Steps * the times of  `Learner` been called\n",
    "        \n",
    "        SGD,RMS,LSTM FROM defination above\n",
    "         but Adam is adopted by pytorch~ This can be improved later'''\n",
    "        f  = self.f \n",
    "        x , W , Y , state =  self.Reset_Or_Reuse(self.x , self.W , self.Y , self.state , num_roll )\n",
    "        B=bfs.BFGS(x, inverse=True)\n",
    "        self.global_loss_graph = 0   #at the beginning of unroll, reset to 0 \n",
    "        optimizer = self.optimizer\n",
    "        update=torch.zeros_like(x,requires_grad=True)\n",
    "        grad_before=torch.zeros_like(x,requires_grad=True)\n",
    "        for i in range(self.train_steps):   \n",
    "            loss = f(W,Y,x)\n",
    "            #self.global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss\n",
    "            self.global_loss_graph += loss\n",
    "\n",
    "            loss.backward(retain_graph=self.retain_graph_flag) # default as False,set to True for LSTMS\n",
    "            g=x.grad.clone()\n",
    "            \n",
    "            input_data=torch.cat((update,g-grad_before),dim=0)\n",
    "            print(update.shape,g.shape)\n",
    "            B.update(update,g-grad_before)\n",
    "            dire=-B.dot(g)\n",
    "            f_new, g_new, t, ls_evals=bfs.strong_wolfe(f,x,1,dire,loss,g)\n",
    "            \n",
    "            update=t*dire\n",
    "            grad_before=g\n",
    "             # f_new, g_new, t, ls_evals = \\\n",
    "             # strong_wolfe(dir_evaluate, x, t, d, f, g, gtd)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            x = x + update\n",
    "#             if self.retain_graph_flag==True:\n",
    "#                 x.retain_grad()\n",
    "#                 update.retain_grad()\n",
    "#             else:\n",
    "#                 x.retain_grad()\n",
    "\n",
    "#         if state is not None:\n",
    "#             self.state = (state[0].detach(),state[1].detach())\n",
    "\n",
    "        return self.losses ,self.global_loss_graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset the values of `W`, `x`, `Y` and `state` for this optimizer\n",
      "torch.Size([1, 10]) torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m Adam \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Adam in Pytorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m SGD_Learner \u001b[38;5;241m=\u001b[39m Learner(f , SGD, STEPS, eval_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,reset_theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m sgd_losses, sgd_sum_loss \u001b[38;5;241m=\u001b[39m \u001b[43mSGD_Learner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 106\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[1;34m(self, num_roll)\u001b[0m\n\u001b[0;32m    104\u001b[0m input_data\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((update,g\u001b[38;5;241m-\u001b[39mgrad_before),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(update\u001b[38;5;241m.\u001b[39mshape,g\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mgrad_before\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m dire\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mB\u001b[38;5;241m.\u001b[39mdot(g)\n\u001b[0;32m    108\u001b[0m f_new, g_new, t, ls_evals\u001b[38;5;241m=\u001b[39mbfs\u001b[38;5;241m.\u001b[39mstrong_wolfe(f,x,\u001b[38;5;241m1\u001b[39m,dire,loss,g)\n",
      "File \u001b[1;32m~\\Desktop\\Project\\Cubic\\torchmin\\bfgs.py:27\u001b[0m, in \u001b[0;36mHessianUpdateStrategy.update\u001b[1;34m(self, s, y)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, y):\n\u001b[1;32m---> 27\u001b[0m     rho_inv \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rho_inv \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-10\u001b[39m:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# curvature is negative; do not update\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "STEPS = 200\n",
    "x = np.arange(STEPS)\n",
    "\n",
    "Adam = 'Adam' # Adam in Pytorch\n",
    "\n",
    "SGD_Learner = Learner(f , SGD, STEPS, eval_flag=True,reset_theta=True)\n",
    "sgd_losses, sgd_sum_loss = SGD_Learner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## L2LG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot4tensorlist(x, losses, label):\n",
    "    y=[i.detach().cpu().numpy() for i in losses]\n",
    "    return plt.plot(x,y,label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Learning_to_learn_global_training(optimizer, global_taining_steps, optimizer_Train_Steps, UnRoll_STEPS, Evaluate_period ,optimizer_lr=0.1):\n",
    "    \"\"\" Training the LSTM optimizer . Learning to learn\n",
    "\n",
    "    Args:   \n",
    "        `optimizer` : DeepLSTMCoordinateWise optimizer model\n",
    "        `global_taining_steps` : how many steps for optimizer training o可以ptimizee\n",
    "        `optimizer_Train_Steps` : how many step for optimizer opimitzing each function sampled from IID.\n",
    "        `UnRoll_STEPS` :: how many steps for LSTM optimizer being unrolled to construct a computing graph to BPTT.\n",
    "    \"\"\"\n",
    "    global_loss_list = []\n",
    "    Total_Num_Unroll = optimizer_Train_Steps // UnRoll_STEPS\n",
    "    adam_global_optimizer = torch.optim.Adam(optimizer.parameters(),lr = optimizer_lr)\n",
    "\n",
    "    LSTM_Learner = Learner(f, optimizer, UnRoll_STEPS, retain_graph_flag=True, reset_theta=True,)\n",
    "  #这里考虑Batchsize代表IID的话，那么就可以不需要每次都重新IID采样\n",
    "  # If regarding `Batchsize` as `IID` ,there is no need for reset the theta\n",
    "  #That is ，reset_function_from_IID_distirbution = False else it is True\n",
    "\n",
    "    best_sum_loss = 999999\n",
    "    best_final_loss = 999999\n",
    "    best_flag = False\n",
    "    for i in range(Global_Train_Steps): \n",
    "\n",
    "        print('\\n========================================> global training steps: {}'.format(i))\n",
    "\n",
    "        for num in range(Total_Num_Unroll):\n",
    "            \n",
    "            start = timer()\n",
    "            _,global_loss = LSTM_Learner(num)   \n",
    "\n",
    "            adam_global_optimizer.zero_grad()\n",
    "            global_loss.backward() \n",
    "       \n",
    "            adam_global_optimizer.step()\n",
    "            # print('xxx',[(z.grad,z.requires_grad) for z in optimizer.lstm.parameters()  ])\n",
    "            global_loss_list.append(global_loss.detach_())\n",
    "            time = timer() - start\n",
    "            #if i % 10 == 0:\n",
    "            print('-> time consuming [{:.1f}s] optimizer train steps :  [{}] | Global_Loss = [{:.1f}]  '\\\n",
    "                  .format(time,(num +1)* UnRoll_STEPS,global_loss,))\n",
    "\n",
    "        if (i + 1) % Evaluate_period == 0:\n",
    "            \n",
    "            best_sum_loss, best_final_loss, best_flag  = evaluate(best_sum_loss,best_final_loss,best_flag , optimizer_lr)\n",
    "    return global_loss_list,best_flag\n",
    "\n",
    "\n",
    "def evaluate(best_sum_loss,best_final_loss, best_flag,lr):\n",
    "    print('\\n --------> evalute the model')\n",
    "\n",
    "    STEPS = 100\n",
    "    x = np.arange(STEPS)\n",
    "    Adam = 'Adam'\n",
    "\n",
    "    LSTM_learner = Learner(f , LSTM_optimizer, STEPS, eval_flag=True,reset_theta=True, retain_graph_flag=True)\n",
    "    \n",
    "    BFGS_Learner = Learner(f , bfgs, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=True,reset_theta=True,)\n",
    "\n",
    "\n",
    "    sgd_losses, sgd_sum_loss = SGD_Learner()\n",
    "    rms_losses, rms_sum_loss = RMS_Learner()\n",
    "    adam_losses, adam_sum_loss = Adam_Learner()\n",
    "    lstm_losses, lstm_sum_loss = LSTM_learner()\n",
    "\n",
    "    p1, = plot4tensorlist(x, sgd_losses, label='SGD')\n",
    "    p2, = plot4tensorlist(x, rms_losses, label='RMS')\n",
    "    p3, = plot4tensorlist(x, adam_losses, label='Adam')\n",
    "    p4, = plot4tensorlist(x, lstm_losses, label='LSTM')\n",
    "    plt.yscale('log')\n",
    "    #plt.legend(handles=[p1, p2, p3, p4])\n",
    "    plt.title('Losses')\n",
    "    plt.pause(1.5)\n",
    "    #plt.show()\n",
    "    #print(\"sum_loss:sgd={},rms={},adam={},lstm={}\".format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))\n",
    "    plt.close() \n",
    "    torch.save(LSTM_optimizer.state_dict(),'current_LSTM_optimizer_ckpt.pth')\n",
    "\n",
    "    try:\n",
    "        best = torch.load('best_loss.txt')\n",
    "    except IOError:\n",
    "        print ('can not find best_loss.txt')\n",
    "        now_sum_loss = lstm_sum_loss.cpu()\n",
    "        now_final_loss = lstm_losses[-1].cpu()\n",
    "        pass\n",
    "    else:\n",
    "        best_sum_loss = best[0].cpu()\n",
    "        best_final_loss = best[1].cpu()\n",
    "        now_sum_loss = lstm_sum_loss.cpu()\n",
    "        now_final_loss = lstm_losses[-1].cpu()\n",
    "\n",
    "        print(\" ==> History: sum loss = [{:.1f}] \\t| final loss = [{:.2f}]\".format(best_sum_loss,best_final_loss))\n",
    "        print(\" ==> Current: sum loss = [{:.1f}] \\t| final loss = [{:.2f}]\".format(now_sum_loss,now_final_loss))\n",
    "\n",
    "    # save the best model according to the conditions below\n",
    "    # there may be several choices to make a trade-off\n",
    "    if now_final_loss < best_final_loss: # and  now_sum_loss < best_sum_loss:\n",
    "        \n",
    "        best_final_loss = now_final_loss\n",
    "        best_sum_loss =  now_sum_loss \n",
    "        \n",
    "        print('\\n\\n===> update new best of final LOSS[{}]: =  {}, best_sum_loss ={}'.format(STEPS, best_final_loss,best_sum_loss))\n",
    "        torch.save(LSTM_optimizer.state_dict(),'best_LSTM_optimizer.pth')\n",
    "        torch.save([best_sum_loss ,best_final_loss,lr ],'best_loss.txt')\n",
    "        best_flag = True\n",
    "        \n",
    "    return best_sum_loss, best_final_loss, best_flag "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## befor lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset the values of `W`, `x`, `Y` and `state` for this optimizer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'clone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m Adam_Learner \u001b[38;5;241m=\u001b[39m Learner(f , Adam, STEPS, eval_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,reset_theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[0;32m     16\u001b[0m LSTM_learner \u001b[38;5;241m=\u001b[39m Learner(f , LSTM_optimizer, STEPS, eval_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,reset_theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,retain_graph_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m sgd_losses, sgd_sum_loss \u001b[38;5;241m=\u001b[39m \u001b[43mSGD_Learner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m rms_losses, rms_sum_loss \u001b[38;5;241m=\u001b[39m RMS_Learner()\n\u001b[0;32m     20\u001b[0m adam_losses, adam_sum_loss \u001b[38;5;241m=\u001b[39m Adam_Learner()\n",
      "Cell \u001b[1;32mIn[19], line 102\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[1;34m(self, num_roll)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_loss_graph \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretain_graph_flag) \u001b[38;5;66;03m# default as False,set to True for LSTMS\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m update, state \u001b[38;5;241m=\u001b[39m optimizer(\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach(), state)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    105\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m update\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clone'"
     ]
    }
   ],
   "source": [
    "    \n",
    "##########################   before learning LSTM optimizer ###############################\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "STEPS = 200\n",
    "x = np.arange(STEPS)\n",
    "\n",
    "Adam = 'Adam' # Adam in Pytorch\n",
    "\n",
    "for _ in range(1): \n",
    "   \n",
    "    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    LSTM_learner = Learner(f , LSTM_optimizer, STEPS, eval_flag=True,reset_theta=True,retain_graph_flag=True)\n",
    "\n",
    "    sgd_losses, sgd_sum_loss = SGD_Learner()\n",
    "    rms_losses, rms_sum_loss = RMS_Learner()\n",
    "    adam_losses, adam_sum_loss = Adam_Learner()\n",
    "    lstm_losses, lstm_sum_loss = LSTM_learner()\n",
    "\n",
    "    p1, = plot4tensorlist(x, sgd_losses, label='SGD')\n",
    "    p2, = plot4tensorlist(x, rms_losses, label='RMS')\n",
    "    p3, = plot4tensorlist(x, adam_losses, label='Adam')\n",
    "    p4, = plot4tensorlist(x, lstm_losses, label='LSTM')\n",
    "    p1.set_dashes([2, 2, 2, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    p2.set_dashes([4, 2, 8, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    p3.set_dashes([3, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    plt.yscale('log')\n",
    "    plt.legend(handles=[p1, p2, p3, p4])\n",
    "    plt.title('Losses')\n",
    "    plt.pause(2.5)\n",
    "    print(\"\\n\\nsum_loss:sgd={},rms={},adam={},lstm={}\".format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))\n",
    "\n",
    "\n",
    "\n",
    "#################### Learning to learn (optimizing optimizer) ######################\n",
    "Global_Train_Steps = 10 #可修改 changeable\n",
    "optimizer_Train_Steps = 100\n",
    "UnRoll_STEPS = 20\n",
    "Evaluate_period = 1 #可修改 changeable\n",
    "optimizer_lr = 0.1 #可修改 changeable\n",
    "global_loss_list ,flag = Learning_to_learn_global_training(   LSTM_optimizer,\n",
    "                                                        Global_Train_Steps,\n",
    "                                                        optimizer_Train_Steps,\n",
    "                                                        UnRoll_STEPS,\n",
    "                                                        Evaluate_period,\n",
    "                                                          optimizer_lr)\n",
    "\n",
    "\n",
    "######################################################################3#\n",
    "##########################   show learning process results \n",
    "#torch.load('best_LSTM_optimizer.pth'))\n",
    "#import numpy as np\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#Global_T = np.arange(len(global_loss_list))\n",
    "#p1, = plt.plot(Global_T, global_loss_list, label='Global_graph_loss')\n",
    "#plt.legend(handles=[p1])\n",
    "#plt.title('Training LSTM optimizer by gradient descent ')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if flag ==True :\n",
    "    print('\\n==== > load best LSTM model')\n",
    "    last_state_dict = copy.deepcopy(LSTM_optimizer.state_dict())\n",
    "    torch.save(LSTM_optimizer.state_dict(),'final_LSTM_optimizer.pth')\n",
    "    LSTM_optimizer.load_state_dict( torch.load('best_LSTM_optimizer.pth'))\n",
    "    \n",
    "LSTM_optimizer.load_state_dict(torch.load('best_LSTM_optimizer.pth'))\n",
    "#LSTM_optimizer.load_state_dict(torch.load('final_LSTM_optimizer.pth'))\n",
    "STEPS = 100\n",
    "x = np.arange(STEPS)\n",
    "\n",
    "Adam = 'Adam'\n",
    "\n",
    "for _ in range(3): #可以多试几次测试实验，LSTM不稳定 for several test,  the trained LSTM is not stable?\n",
    "    \n",
    "    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=True,reset_theta=True,)\n",
    "    LSTM_learner = Learner(f , LSTM_optimizer, STEPS, eval_flag=True,reset_theta=True,retain_graph_flag=True)\n",
    "\n",
    "    \n",
    "    sgd_losses, sgd_sum_loss = SGD_Learner()\n",
    "    rms_losses, rms_sum_loss = RMS_Learner()\n",
    "    adam_losses, adam_sum_loss = Adam_Learner()\n",
    "    lstm_losses, lstm_sum_loss = LSTM_learner()\n",
    "\n",
    "    p1, = plot4tensorlist(x, sgd_losses, label='SGD')\n",
    "    p2, = plot4tensorlist(x, rms_losses, label='RMS')\n",
    "    p3, = plot4tensorlist(x, adam_losses, label='Adam')\n",
    "    p4, = plot4tensorlist(x, lstm_losses, label='LSTM')\n",
    "    p1.set_dashes([2, 2, 2, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    p2.set_dashes([4, 2, 8, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    p3.set_dashes([3, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    #p4.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "    plt.yscale('log')\n",
    "    plt.legend(handles=[p1, p2, p3, p4])\n",
    "    plt.title('Losses')\n",
    "    plt.show()\n",
    "    print(\"\\n\\nsum_loss:sgd={},rms={},adam={},lstm={}\".format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "pytor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
